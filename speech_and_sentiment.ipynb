{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862d25dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and install required packages if needed\n",
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "\n",
    "required_packages = [\n",
    "    'nltk',\n",
    "    'numpy',\n",
    "    'pandas',\n",
    "    'matplotlib',\n",
    "    'seaborn',\n",
    "    'textblob',\n",
    "    'wordcloud',\n",
    "    'scikit-learn',\n",
    "    'gensim',\n",
    "    'SpeechRecognition'\n",
    "]\n",
    "\n",
    "def install_packages(packages):\n",
    "    \"\"\"Install packages using pip\"\"\"\n",
    "    for package in packages:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"{package} installed successfully!\")\n",
    "\n",
    "# Check for missing packages\n",
    "installed_packages = {pkg.key for pkg in pkg_resources.working_set}\n",
    "missing_packages = [pkg for pkg in required_packages if pkg.lower() not in installed_packages]\n",
    "\n",
    "# Install missing packages\n",
    "if missing_packages:\n",
    "    print(f\"The following packages are missing and will be installed: {', '.join(missing_packages)}\")\n",
    "    try:\n",
    "        install_packages(missing_packages)\n",
    "        print(\"All required packages are now installed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error installing packages: {e}\")\n",
    "        print(\"Please install the missing packages manually before running this notebook.\")\n",
    "else:\n",
    "    print(\"All required packages are already installed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525cfd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "# Core libraries\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download NLTK data with error handling\n",
    "def download_nltk_data():\n",
    "    resources = ['punkt', 'stopwords', 'wordnet', 'vader_lexicon']\n",
    "    for resource in resources:\n",
    "        try:\n",
    "            print(f\"Downloading NLTK resource: {resource}...\")\n",
    "            nltk.download(resource, quiet=True)\n",
    "            print(f\"Successfully downloaded {resource}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {resource}: {e}\")\n",
    "            print(f\"You might need to manually download this resource using nltk.download('{resource}')\")\n",
    "\n",
    "# Call the function to download NLTK data\n",
    "download_nltk_data()\n",
    "\n",
    "# NLTK specific imports\n",
    "try:\n",
    "    from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "    from nltk.probability import FreqDist\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing NLTK modules: {e}\")\n",
    "    print(\"Make sure all NLTK resources are properly downloaded.\")\n",
    "\n",
    "# Import TextBlob\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Visualization\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Topic Modeling\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Speech Recognition\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('ggplot')\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df62b43d",
   "metadata": {},
   "source": [
    "# Speech and Sentiment Analysis Project\n",
    "\n",
    "This notebook demonstrates various Natural Language Processing (NLP) techniques using Python libraries. We'll cover:\n",
    "\n",
    "1. **Speech-to-Text Conversion**: Convert spoken language to text using SpeechRecognition\n",
    "2. **Text Preprocessing**: Clean and normalize text data\n",
    "3. **Exploratory Text Analysis**: Analyze text statistics and visualize patterns\n",
    "4. **Sentiment Analysis**: Determine sentiment using multiple approaches\n",
    "5. **Topic Modeling**: Extract topics from text collections\n",
    "6. **Text Classification**: Build machine learning models to classify text\n",
    "\n",
    "Let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae92127",
   "metadata": {},
   "source": [
    "## 1. Speech-to-Text Conversion\n",
    "\n",
    "First, let's set up the speech recognition functionality. This allows us to convert spoken language to text for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d31951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_speech():\n",
    "    \"\"\"Record speech and convert to text using Google's Speech Recognition API\"\"\"\n",
    "    \n",
    "    # Check if speech recognition is available\n",
    "    try:\n",
    "        # Initialize recognizer\n",
    "        recognizer = sr.Recognizer()\n",
    "        \n",
    "        # Check if microphone is available\n",
    "        try:\n",
    "            mics = sr.Microphone.list_microphone_names()\n",
    "            if not mics:\n",
    "                print(\"No microphones found. Make sure your microphone is connected.\")\n",
    "                return \"\"\n",
    "            print(f\"Available microphones: {len(mics)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error checking microphones: {e}\")\n",
    "        \n",
    "        print(\"Please speak something...\")\n",
    "        \n",
    "        # Try to capture speech from microphone\n",
    "        try:\n",
    "            with sr.Microphone() as source:\n",
    "                print(\"Adjusting for ambient noise...\")\n",
    "                recognizer.adjust_for_ambient_noise(source, duration=1)\n",
    "                print(\"Listening...\")\n",
    "                audio = recognizer.listen(source, timeout=5)\n",
    "                print(\"Processing...\")\n",
    "                \n",
    "            # Use Google Speech Recognition to convert audio to text\n",
    "            text = recognizer.recognize_google(audio)\n",
    "            print(f\"You said: {text}\")\n",
    "            return text\n",
    "        \n",
    "        except sr.WaitTimeoutError:\n",
    "            print(\"No speech detected within the timeout period.\")\n",
    "            return \"\"\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Could not understand audio\")\n",
    "            return \"\"\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Could not request results from Google Speech Recognition service; {e}\")\n",
    "            return \"\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error during speech recognition: {e}\")\n",
    "            return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up speech recognition: {e}\")\n",
    "        print(\"Make sure SpeechRecognition and PyAudio are installed correctly.\")\n",
    "        return \"\"\n",
    "\n",
    "# Note: Uncomment to use the speech recognition\n",
    "# speech_text = recognize_speech()\n",
    "\n",
    "# For demonstration purposes, we'll use sample texts\n",
    "sample_texts = [\n",
    "    \"I absolutely loved the movie. The acting was superb and the plot was engaging throughout.\",\n",
    "    \"The customer service was terrible. I waited for hours and still did not get help.\",\n",
    "    \"The product is okay, but it could be better. Some features work well while others need improvement.\",\n",
    "    \"I'm really excited about the upcoming conference. The speakers lineup looks amazing!\",\n",
    "    \"The food at the restaurant was mediocre, but the service was excellent.\"\n",
    "]\n",
    "\n",
    "# Create a DataFrame with sample texts for demonstration\n",
    "df = pd.DataFrame(sample_texts, columns=['text'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5ee225",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing\n",
    "\n",
    "Before analyzing text data, we need to clean and normalize it. This typically involves:\n",
    "\n",
    "- Converting to lowercase\n",
    "- Removing punctuation and special characters\n",
    "- Tokenization (splitting text into words)\n",
    "- Removing stopwords (common words like 'and', 'the', etc.)\n",
    "- Stemming or lemmatization (reducing words to their root form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8464878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stopwords, stemmer and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess text by applying various cleaning operations\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatize tokens\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return {\n",
    "        'original_text': text,\n",
    "        'tokens': tokens,\n",
    "        'lemmatized_tokens': lemmatized_tokens,\n",
    "        'processed_text': ' '.join(lemmatized_tokens)\n",
    "    }\n",
    "\n",
    "# Apply preprocessing to our sample texts\n",
    "df['preprocessed'] = df['text'].apply(lambda x: preprocess_text(x))\n",
    "\n",
    "# Extract processed text for further analysis\n",
    "df['processed_text'] = df['preprocessed'].apply(lambda x: x['processed_text'])\n",
    "\n",
    "# Display results\n",
    "for i, row in df.iterrows():\n",
    "    print(f\"Original: {row['text']}\")\n",
    "    print(f\"Processed: {row['processed_text']}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec167bb",
   "metadata": {},
   "source": [
    "## 3. Exploratory Text Analysis\n",
    "\n",
    "Now let's explore the text data by analyzing word frequencies, text statistics, and creating visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f95985a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_stats(text_series):\n",
    "    \"\"\"Calculate various statistics for a series of texts\"\"\"\n",
    "    \n",
    "    # Text length statistics\n",
    "    text_lengths = text_series.str.len()\n",
    "    \n",
    "    # Word count statistics\n",
    "    word_counts = text_series.apply(lambda x: len(word_tokenize(x)))\n",
    "    \n",
    "    # Sentence count statistics\n",
    "    sentence_counts = text_series.apply(lambda x: len(sent_tokenize(x)))\n",
    "    \n",
    "    # Average word length\n",
    "    avg_word_lengths = text_series.apply(lambda x: np.mean([len(word) for word in word_tokenize(x)]) if word_tokenize(x) else 0)\n",
    "    \n",
    "    # Create a DataFrame with statistics\n",
    "    stats_df = pd.DataFrame({\n",
    "        'text_length': text_lengths,\n",
    "        'word_count': word_counts,\n",
    "        'sentence_count': sentence_counts,\n",
    "        'avg_word_length': avg_word_lengths\n",
    "    })\n",
    "    \n",
    "    return stats_df\n",
    "\n",
    "# Get text statistics\n",
    "text_stats = get_text_stats(df['text'])\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"Text Statistics Summary:\")\n",
    "print(text_stats.describe())\n",
    "\n",
    "# Create visualizations for text statistics\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.histplot(text_stats['text_length'], kde=True)\n",
    "plt.title('Text Length Distribution')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.histplot(text_stats['word_count'], kde=True)\n",
    "plt.title('Word Count Distribution')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.histplot(text_stats['sentence_count'], kde=True)\n",
    "plt.title('Sentence Count Distribution')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.histplot(text_stats['avg_word_length'], kde=True)\n",
    "plt.title('Average Word Length Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Word frequency analysis\n",
    "all_words = ' '.join(df['processed_text']).split()\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "# Get top N most common words\n",
    "top_n = 20\n",
    "most_common_words = word_freq.most_common(top_n)\n",
    "\n",
    "# Create DataFrame for plotting\n",
    "common_words_df = pd.DataFrame(most_common_words, columns=['word', 'count'])\n",
    "\n",
    "# Plot word frequencies\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='word', y='count', data=common_words_df)\n",
    "plt.title(f'Top {top_n} Most Common Words')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create word cloud\n",
    "wc = WordCloud(width=800, height=400, background_color='white', max_words=100).generate(' '.join(df['processed_text']))\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of Processed Text')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd4f3de",
   "metadata": {},
   "source": [
    "## 4. Sentiment Analysis\n",
    "\n",
    "Sentiment analysis determines whether a piece of text expresses positive, negative, or neutral sentiment. We'll use multiple approaches for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ea7638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment_vader(text):\n",
    "    \"\"\"Analyze sentiment using NLTK's VADER SentimentIntensityAnalyzer\"\"\"\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment_scores = sia.polarity_scores(text)\n",
    "    \n",
    "    # Determine overall sentiment based on compound score\n",
    "    if sentiment_scores['compound'] >= 0.05:\n",
    "        sentiment = 'Positive'\n",
    "    elif sentiment_scores['compound'] <= -0.05:\n",
    "        sentiment = 'Negative'\n",
    "    else:\n",
    "        sentiment = 'Neutral'\n",
    "    \n",
    "    return {\n",
    "        'neg': sentiment_scores['neg'],\n",
    "        'neu': sentiment_scores['neu'],\n",
    "        'pos': sentiment_scores['pos'],\n",
    "        'compound': sentiment_scores['compound'],\n",
    "        'sentiment': sentiment\n",
    "    }\n",
    "\n",
    "def analyze_sentiment_textblob(text):\n",
    "    \"\"\"Analyze sentiment using TextBlob\"\"\"\n",
    "    analysis = TextBlob(text)\n",
    "    \n",
    "    # TextBlob's polarity ranges from -1 (negative) to 1 (positive)\n",
    "    polarity = analysis.sentiment.polarity\n",
    "    subjectivity = analysis.sentiment.subjectivity\n",
    "    \n",
    "    # Determine sentiment based on polarity\n",
    "    if polarity > 0.05:\n",
    "        sentiment = 'Positive'\n",
    "    elif polarity < -0.05:\n",
    "        sentiment = 'Negative'\n",
    "    else:\n",
    "        sentiment = 'Neutral'\n",
    "    \n",
    "    return {\n",
    "        'polarity': polarity,\n",
    "        'subjectivity': subjectivity,\n",
    "        'sentiment': sentiment\n",
    "    }\n",
    "\n",
    "# Apply sentiment analysis to our texts\n",
    "df['vader_sentiment'] = df['text'].apply(analyze_sentiment_vader)\n",
    "df['textblob_sentiment'] = df['text'].apply(analyze_sentiment_textblob)\n",
    "\n",
    "# Extract sentiment labels for comparison\n",
    "df['vader_label'] = df['vader_sentiment'].apply(lambda x: x['sentiment'])\n",
    "df['textblob_label'] = df['textblob_sentiment'].apply(lambda x: x['sentiment'])\n",
    "\n",
    "# Display results\n",
    "print(\"Sentiment Analysis Results:\")\n",
    "for i, row in df.iterrows():\n",
    "    print(f\"Text: {row['text']}\")\n",
    "    print(f\"VADER: {row['vader_label']} (compound: {row['vader_sentiment']['compound']:.3f})\")\n",
    "    print(f\"TextBlob: {row['textblob_label']} (polarity: {row['textblob_sentiment']['polarity']:.3f}, subjectivity: {row['textblob_sentiment']['subjectivity']:.3f})\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Visualize sentiment distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x='vader_label', data=df)\n",
    "plt.title('Sentiment Distribution (VADER)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(x='textblob_label', data=df)\n",
    "plt.title('Sentiment Distribution (TextBlob)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare VADER compound scores with TextBlob polarity\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(\n",
    "    [x['compound'] for x in df['vader_sentiment']],\n",
    "    [x['polarity'] for x in df['textblob_sentiment']],\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.grid(True)\n",
    "plt.xlabel('VADER Compound Score')\n",
    "plt.ylabel('TextBlob Polarity')\n",
    "plt.title('VADER vs. TextBlob Sentiment Scores')\n",
    "plt.axhline(y=0, color='r', linestyle='-', alpha=0.3)\n",
    "plt.axvline(x=0, color='r', linestyle='-', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cca978b",
   "metadata": {},
   "source": [
    "## 5. Topic Modeling\n",
    "\n",
    "Topic modeling is a technique to discover abstract topics in a collection of documents. We'll use Latent Dirichlet Allocation (LDA) for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a338061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_lda_topic_modeling(texts, num_topics=3, num_words=5):\n",
    "    \"\"\"Perform LDA topic modeling on a list of preprocessed texts\"\"\"\n",
    "    # Convert texts to list of lists of words\n",
    "    texts_as_lists = [text.split() for text in texts]\n",
    "    \n",
    "    # Create dictionary\n",
    "    dictionary = corpora.Dictionary(texts_as_lists)\n",
    "    \n",
    "    # Create document-term matrix\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts_as_lists]\n",
    "    \n",
    "    # Build LDA model\n",
    "    lda_model = LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=dictionary,\n",
    "        num_topics=num_topics,\n",
    "        random_state=42,\n",
    "        passes=10\n",
    "    )\n",
    "    \n",
    "    # Extract topics\n",
    "    topics = lda_model.print_topics(num_words=num_words)\n",
    "    \n",
    "    # Get document-topic distribution\n",
    "    doc_topics = [lda_model[doc] for doc in corpus]\n",
    "    \n",
    "    return {\n",
    "        'model': lda_model,\n",
    "        'topics': topics,\n",
    "        'doc_topics': doc_topics,\n",
    "        'corpus': corpus,\n",
    "        'dictionary': dictionary\n",
    "    }\n",
    "\n",
    "# For robust topic modeling, we would need more documents\n",
    "# For demonstration purposes, let's create some additional texts\n",
    "additional_texts = [\n",
    "    \"The smartphone has an excellent camera and battery life.\",\n",
    "    \"This laptop performance is outstanding for gaming and productivity.\",\n",
    "    \"The hotel room was spacious with a beautiful view of the ocean.\",\n",
    "    \"The conference speakers provided valuable insights on artificial intelligence.\",\n",
    "    \"This restaurant serves delicious Italian cuisine with excellent wine pairings.\"\n",
    "]\n",
    "\n",
    "# Safely preprocess additional texts\n",
    "preprocessed_additional_texts = []\n",
    "for text in additional_texts:\n",
    "    try:\n",
    "        preprocessed_additional_texts.append(preprocess_text(text)['processed_text'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error preprocessing text: {e}\")\n",
    "        preprocessed_additional_texts.append(\"\")\n",
    "\n",
    "# Combine with our original texts\n",
    "all_texts = list(df['processed_text']) + preprocessed_additional_texts\n",
    "\n",
    "# Remove any empty strings that might have resulted from preprocessing errors\n",
    "all_texts = [text for text in all_texts if text]\n",
    "\n",
    "# Check if we have enough texts for topic modeling\n",
    "if len(all_texts) < 3:\n",
    "    print(\"Not enough texts for meaningful topic modeling. Need at least 3 documents.\")\n",
    "    num_topics = min(3, len(all_texts))\n",
    "else:\n",
    "    num_topics = 3\n",
    "\n",
    "# Perform topic modeling\n",
    "lda_results = perform_lda_topic_modeling(all_texts, num_topics=num_topics)\n",
    "\n",
    "# Display topics\n",
    "print(f\"Top {num_topics} Topics:\")\n",
    "for i, topic in enumerate(lda_results['topics']):\n",
    "    print(f\"Topic #{i+1}: {topic}\")\n",
    "    \n",
    "# Display document-topic distribution\n",
    "print(\"\\nDocument-Topic Distribution:\")\n",
    "for i, doc_topic in enumerate(lda_results['doc_topics']):\n",
    "    # Sort topic distribution by weight\n",
    "    sorted_topics = sorted(doc_topic, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get most dominant topic\n",
    "    dominant_topic = sorted_topics[0] if sorted_topics else (0, 0)\n",
    "    \n",
    "    # Get original text (cutting it if too long)\n",
    "    text = all_texts[i]\n",
    "    if len(text) > 50:\n",
    "        text = text[:50] + '...'\n",
    "        \n",
    "    print(f\"Document #{i+1}: Dominant Topic: #{dominant_topic[0]+1} ({dominant_topic[1]:.2f})\")\n",
    "    print(f\"   Text: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80785219",
   "metadata": {},
   "source": [
    "## 6. Text Classification\n",
    "\n",
    "Now we'll use machine learning to build a text classifier based on sentiment. Since we've already done sentiment analysis, we'll use those labels as our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a19783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we have a small dataset, let's create more synthetic examples for demonstration purposes\n",
    "def generate_synthetic_examples():\n",
    "    positive_examples = [\n",
    "        \"I really enjoyed this product, it exceeded my expectations.\",\n",
    "        \"The service was outstanding, I'll definitely come back.\",\n",
    "        \"This is by far the best experience I've had with any company.\",\n",
    "        \"The staff was friendly and very helpful throughout the process.\",\n",
    "        \"Excellent quality and fast delivery, highly recommended!\",\n",
    "        \"I'm very satisfied with my purchase, great value for money.\",\n",
    "        \"The app is intuitive and has all the features I needed.\",\n",
    "        \"Their customer support resolved my issue promptly and professionally.\"\n",
    "    ]\n",
    "    \n",
    "    neutral_examples = [\n",
    "        \"The product works as expected, nothing special though.\",\n",
    "        \"It's an average service, neither good nor bad.\",\n",
    "        \"The quality is acceptable for the price point.\",\n",
    "        \"I received exactly what was advertised, no surprises.\",\n",
    "        \"The performance is standard, meets basic requirements.\",\n",
    "        \"It's okay for occasional use but not for heavy usage.\",\n",
    "        \"The interface is functional but could use some improvements.\",\n",
    "        \"Not bad, but I've seen better options in this price range.\"\n",
    "    ]\n",
    "    \n",
    "    negative_examples = [\n",
    "        \"This product is a complete waste of money.\",\n",
    "        \"The customer service was rude and unhelpful.\",\n",
    "        \"I'm very disappointed with the quality, it broke after a week.\",\n",
    "        \"Definitely not worth the price, I regret this purchase.\",\n",
    "        \"The app is buggy and crashes constantly, terrible user experience.\",\n",
    "        \"I waited for hours and still didn't get proper assistance.\",\n",
    "        \"Would not recommend to anyone, stay away from this service.\",\n",
    "        \"The worst experience I've ever had with an online store.\"\n",
    "    ]\n",
    "    \n",
    "    all_examples = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for examples, sentiment in zip([positive_examples, neutral_examples, negative_examples], \n",
    "                                 ['Positive', 'Neutral', 'Negative']):\n",
    "        all_examples.extend(examples)\n",
    "        all_labels.extend([sentiment] * len(examples))\n",
    "    \n",
    "    # Create DataFrame\n",
    "    synthetic_df = pd.DataFrame({\n",
    "        'text': all_examples,\n",
    "        'sentiment': all_labels\n",
    "    })\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    synthetic_df['processed_text'] = synthetic_df['text'].apply(lambda x: preprocess_text(x)['processed_text'])\n",
    "    \n",
    "    return synthetic_df\n",
    "\n",
    "# Generate synthetic dataset\n",
    "synthetic_df = generate_synthetic_examples()\n",
    "\n",
    "# Display sample of the synthetic dataset\n",
    "print(f\"Synthetic dataset shape: {synthetic_df.shape}\")\n",
    "print(synthetic_df.head())\n",
    "\n",
    "# Feature extraction using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = tfidf_vectorizer.fit_transform(synthetic_df['processed_text'])\n",
    "y = synthetic_df['sentiment']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "# Train a Multinomial Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_nb = nb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Multinomial Naive Bayes Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_nb):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "\n",
    "# Train a Logistic Regression classifier\n",
    "lr_classifier = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lr = lr_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nLogistic Regression Results:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_lr):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "# Plot confusion matrices\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_nb), annot=True, fmt='d', ax=ax1, \n",
    "            cmap='Blues', xticklabels=nb_classifier.classes_, yticklabels=nb_classifier.classes_)\n",
    "ax1.set_title('Confusion Matrix - Naive Bayes')\n",
    "ax1.set_xlabel('Predicted')\n",
    "ax1.set_ylabel('Actual')\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred_lr), annot=True, fmt='d', ax=ax2, \n",
    "            cmap='Blues', xticklabels=lr_classifier.classes_, yticklabels=lr_classifier.classes_)\n",
    "ax2.set_title('Confusion Matrix - Logistic Regression')\n",
    "ax2.set_xlabel('Predicted')\n",
    "ax2.set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfcbf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_new_text(text, tfidf_vectorizer=None, classifier=None):\n",
    "    \"\"\"Analyze a new text input using our NLP pipeline\"\"\"\n",
    "    \n",
    "    print(\"\\n==== Text Analysis ====\\n\")\n",
    "    print(f\"Original text: {text}\")\n",
    "    \n",
    "    # Preprocess text\n",
    "    preprocessed = preprocess_text(text)\n",
    "    processed_text = preprocessed['processed_text']\n",
    "    print(f\"\\nProcessed text: {processed_text}\")\n",
    "    \n",
    "    # Get text statistics\n",
    "    num_words = len(word_tokenize(text))\n",
    "    num_chars = len(text)\n",
    "    num_sentences = len(sent_tokenize(text))\n",
    "    \n",
    "    print(f\"\\nText Statistics:\")\n",
    "    print(f\"- Characters: {num_chars}\")\n",
    "    print(f\"- Words: {num_words}\")\n",
    "    print(f\"- Sentences: {num_sentences}\")\n",
    "    print(f\"- Average word length: {np.mean([len(word) for word in word_tokenize(text)]):.2f}\")\n",
    "    \n",
    "    # Sentiment analysis\n",
    "    vader_sentiment = analyze_sentiment_vader(text)\n",
    "    textblob_sentiment = analyze_sentiment_textblob(text)\n",
    "    \n",
    "    print(f\"\\nSentiment Analysis:\")\n",
    "    print(f\"- VADER: {vader_sentiment['sentiment']} (compound: {vader_sentiment['compound']:.3f})\")\n",
    "    print(f\"- TextBlob: {textblob_sentiment['sentiment']} (polarity: {textblob_sentiment['polarity']:.3f}, \"\n",
    "          f\"subjectivity: {textblob_sentiment['subjectivity']:.3f})\")\n",
    "    \n",
    "    # Create a basic result dictionary\n",
    "    result = {\n",
    "        'original_text': text,\n",
    "        'processed_text': processed_text,\n",
    "        'statistics': {\n",
    "            'chars': num_chars,\n",
    "            'words': num_words,\n",
    "            'sentences': num_sentences\n",
    "        },\n",
    "        'vader_sentiment': vader_sentiment,\n",
    "        'textblob_sentiment': textblob_sentiment\n",
    "    }\n",
    "    \n",
    "    # Only perform ML classification if vectorizer and classifier are provided\n",
    "    if tfidf_vectorizer is not None and classifier is not None:\n",
    "        try:\n",
    "            # Transform text using the vectorizer\n",
    "            text_vectorized = tfidf_vectorizer.transform([processed_text])\n",
    "            predicted_sentiment = classifier.predict(text_vectorized)[0]\n",
    "            probabilities = classifier.predict_proba(text_vectorized)[0]\n",
    "            \n",
    "            print(f\"\\nML Classification:\")\n",
    "            print(f\"- Predicted sentiment: {predicted_sentiment}\")\n",
    "            print(f\"- Prediction probabilities:\")\n",
    "            for i, label in enumerate(classifier.classes_):\n",
    "                print(f\"  - {label}: {probabilities[i]:.4f}\")\n",
    "                \n",
    "            # Add ML classification to result\n",
    "            result['ml_classification'] = {\n",
    "                'prediction': predicted_sentiment,\n",
    "                'probabilities': {label: prob for label, prob in zip(classifier.classes_, probabilities)}\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError during ML classification: {e}\")\n",
    "    else:\n",
    "        print(\"\\nML Classification: Skipped (vectorizer or classifier not provided)\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage with a new text\n",
    "new_text = \"I just received my new headphones and they sound absolutely amazing. The noise cancellation is effective and the battery lasts all day. Highly recommended!\"\n",
    "\n",
    "try:\n",
    "    # Try to use the previously defined vectorizer and classifier if they exist\n",
    "    analysis_result = analyze_new_text(new_text, tfidf_vectorizer, lr_classifier)\n",
    "except NameError as e:\n",
    "    # If they don't exist, just run without ML classification\n",
    "    print(f\"Note: {e}. Running without ML classification.\")\n",
    "    analysis_result = analyze_new_text(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38a0e66",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "In this project, we've explored various NLP techniques using Python libraries:\n",
    "\n",
    "1. **Speech-to-Text Conversion**: We set up functionality to convert spoken language to text using SpeechRecognition.\n",
    "\n",
    "2. **Text Preprocessing**: We implemented a comprehensive text preprocessing pipeline including tokenization, stopword removal, and lemmatization.\n",
    "\n",
    "3. **Exploratory Text Analysis**: We analyzed text statistics and created visualizations to understand our text data better.\n",
    "\n",
    "4. **Sentiment Analysis**: We compared different sentiment analysis approaches (VADER and TextBlob).\n",
    "\n",
    "5. **Topic Modeling**: We used LDA to discover abstract topics in a collection of documents.\n",
    "\n",
    "6. **Text Classification**: We built machine learning models to classify text based on sentiment.\n",
    "\n",
    "This project demonstrates how NLP can be used to extract insights from text data. You can extend this project by:\n",
    "\n",
    "- Incorporating more advanced NLP techniques like named entity recognition\n",
    "- Using more sophisticated models like transformers (BERT, GPT, etc.) for sentiment analysis\n",
    "- Creating a real-time speech processing application\n",
    "- Building a larger dataset for more robust model training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
